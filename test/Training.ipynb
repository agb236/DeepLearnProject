{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b02612c6-d42e-4773-82c5-1d5c03116e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from IPython.display import Audio\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2Model, Wav2Vec2ForSequenceClassification\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from torch.optim import AdamW\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3c69c52-cfb7-4401-80a3-0e4305477140",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioMNISTDataset(Dataset):\n",
    "    def __init__(self, root_dir, processor, split=\"train\", test_size=0.2, val_size=0.1, target_sample_rate=16000, random_seed=42):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): Root directory for AudioMNIST (e.g., '/AudioMNIST/data/').\n",
    "            processor (Wav2Vec2Processor): Processor for Wav2Vec2 model.\n",
    "            split (str): One of \"train\", \"val\", or \"test\".\n",
    "            test_size (float): Proportion of the data to use for testing.\n",
    "            val_size (float): Proportion of the training data to use for validation.\n",
    "            target_sample_rate (int): Sample rate required by Wav2Vec2.\n",
    "            random_seed (int): Seed for reproducibility of splits.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.processor = processor\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "        self.split = split\n",
    "        self.audio_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        # Load all file paths and labels\n",
    "        speaker_dirs = [d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n",
    "        all_files = []\n",
    "        all_labels = []\n",
    "\n",
    "        for speaker_id in speaker_dirs:\n",
    "            speaker_dir = os.path.join(root_dir, speaker_id)\n",
    "            for filename in os.listdir(speaker_dir):\n",
    "                if filename.endswith(\".wav\"):\n",
    "                    # Extract digit from filename format '{digit}_{speaker_id}_{sample_no}.wav'\n",
    "                    digit = int(filename.split(\"_\")[0])\n",
    "                    filepath = os.path.join(speaker_dir, filename)\n",
    "                    all_files.append(filepath)\n",
    "                    all_labels.append(digit)\n",
    "\n",
    "        # Split into train, val, test based on speakers for non-overlapping splits\n",
    "        train_files, test_files, train_labels, test_labels = train_test_split(\n",
    "            all_files, all_labels, test_size=test_size, random_state=random_seed, stratify=all_labels\n",
    "        )\n",
    "        train_files, val_files, train_labels, val_labels = train_test_split(\n",
    "            train_files, train_labels, test_size=val_size, random_state=random_seed, stratify=train_labels\n",
    "        )\n",
    "\n",
    "        \n",
    "        # Assign split-specific files and labels\n",
    "        if split == \"train\":\n",
    "            self.audio_paths, self.labels = train_files, train_labels\n",
    "        elif split == \"val\":\n",
    "            self.audio_paths, self.labels = val_files, val_labels\n",
    "        elif split == \"test\":\n",
    "            self.audio_paths, self.labels = test_files, test_labels\n",
    "        else:\n",
    "            raise ValueError(\"split should be one of 'train', 'val', or 'test'.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load the audio file\n",
    "        audio_path = self.audio_paths[idx]\n",
    "        waveform, sample_rate = torchaudio.load(audio_path)\n",
    "\n",
    "        # Resample if necessary\n",
    "        if sample_rate != self.target_sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=self.target_sample_rate)\n",
    "            waveform = resampler(waveform)\n",
    "\n",
    "        # Process waveform with Wav2Vec2 processor\n",
    "        inputs = self.processor(waveform.squeeze().numpy(), sampling_rate=self.target_sample_rate, return_tensors=\"pt\")\n",
    "        \n",
    "        # Retrieve the input values and add label\n",
    "        input_values = inputs.input_values.squeeze()\n",
    "        label = torch.tensor(self.labels[idx])\n",
    "        \n",
    "        return input_values, label\n",
    "        \n",
    "def create_dataloaders(root_dir, processor, batch_size=16, test_size=0.2, val_size=0.1, random_seed=42):\n",
    "    \"\"\"\n",
    "    Create DataLoaders for the train, validation, and test splits of the AudioMNIST dataset.\n",
    "    \n",
    "    Args:\n",
    "        root_dir (str): Root directory containing the AudioMNIST data.\n",
    "        processor (Wav2Vec2Processor): Wav2Vec2 processor.\n",
    "        batch_size (int): Batch size for DataLoaders.\n",
    "        test_size (float): Proportion of data to use for testing.\n",
    "        val_size (float): Proportion of training data to use for validation.\n",
    "        random_seed (int): Random seed for reproducibility.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing DataLoaders for 'train', 'val', and 'test' splits.\n",
    "    \"\"\"\n",
    "    train_dataset = AudioMNISTDataset(root_dir, processor, split=\"train\", test_size=test_size, val_size=val_size, random_seed=random_seed)\n",
    "    val_dataset = AudioMNISTDataset(root_dir, processor, split=\"val\", test_size=test_size, val_size=val_size, random_seed=random_seed)\n",
    "    test_dataset = AudioMNISTDataset(root_dir, processor, split=\"test\", test_size=test_size, val_size=val_size, random_seed=random_seed)\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "    \n",
    "    return {\"train\": train_loader, \"val\": val_loader, \"test\": test_loader}\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to pad input_values and stack labels for batch processing.\n",
    "    \"\"\"\n",
    "    input_values = [item[0] for item in batch]\n",
    "    labels = torch.stack([item[1] for item in batch])\n",
    "\n",
    "    # Pad sequences to the max length in the batch\n",
    "    input_values = torch.nn.utils.rnn.pad_sequence(input_values, batch_first=True)\n",
    "\n",
    "    return input_values, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d8f5534-1575-4fbc-929c-794171f2055f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(model, train_loader, val_loader, optimizer, loss_fn, epochs=3):\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # Ensure the model is in training mode\n",
    "        total_loss = 0  # Track total loss for the epoch\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "\n",
    "        # Iterate through the training data\n",
    "        for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\"):\n",
    "            input_values, labels = batch\n",
    "            input_values = input_values.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_values, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "\n",
    "        # Print average loss and accuracy for training\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        train_accuracy = correct_predictions / total_predictions * 100\n",
    "        print(f\"Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "        # Validation step after each epoch\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        val_loss = 0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "\n",
    "        with torch.no_grad():  # No need to compute gradients during validation\n",
    "            for batch in tqdm(val_loader, desc=f\"Validating Epoch {epoch+1}\"):\n",
    "                input_values, labels = batch\n",
    "                input_values = input_values.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(input_values, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                logits = outputs.logits\n",
    "\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                # Calculate accuracy\n",
    "                _, predicted = torch.max(logits, 1)\n",
    "                correct_predictions += (predicted == labels).sum().item()\n",
    "                total_predictions += labels.size(0)\n",
    "\n",
    "                # Print predictions for each file in the batch\n",
    "                #for i in range(len(labels)):\n",
    "                #    print(f\"Predicted: {predicted[i].item()}, True Label: {labels[i].item()}\")\n",
    "\n",
    "        # Print average validation loss and accuracy\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_accuracy = correct_predictions / total_predictions * 100\n",
    "        print(f\"Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a95ba13-b38a-47fd-a080-da3e170a6aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader):\n",
    "    # Validation step after each epoch\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "\n",
    "        with torch.no_grad():  # No need to compute gradients during validation\n",
    "            for batch in tqdm(test_loader, desc=f\"Test\"):\n",
    "                input_values, labels = batch\n",
    "                input_values = input_values.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(input_values, labels=labels)\n",
    "                logits = outputs.logits\n",
    "\n",
    "                # Calculate accuracy\n",
    "                _, predicted = torch.max(logits, 1)\n",
    "                correct_predictions += (predicted == labels).sum().item()\n",
    "                total_predictions += labels.size(0)\n",
    "\n",
    "        # Print average validation loss and accuracy\n",
    "        test_accuracy = correct_predictions / total_predictions * 100\n",
    "        print(f\"Test Accuracy: {test_accuracy:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316f0413-10d0-4c3b-bd33-d785b30df5ee",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ce234d0-9cf6-4061-84aa-dced0b6a547b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/zhome/e1/b/168603/venv_1/lib/python3.11/site-packages/transformers/configuration_utils.py:306: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "dataloaders = create_dataloaders(\"AudioMNIST/data/\", processor, batch_size=16)\n",
    "\n",
    "train_loader = dataloaders[\"train\"]\n",
    "val_loader = dataloaders[\"val\"]\n",
    "test_loader = dataloaders[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a29c9d-e975-4f9e-a935-35175c00c7d7",
   "metadata": {},
   "source": [
    "## Training class head for model for seq classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82dde64d-1ede-4b53-951f-60378771a1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/zhome/e1/b/168603/venv_1/lib/python3.11/site-packages/transformers/configuration_utils.py:306: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Training Epoch 1: 100%|█████████████████████████████████████████████████████████████| 1350/1350 [10:10<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.1106, Train Accuracy: 64.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 1: 100%|█████████████████████████████████████████████████████████████| 150/150 [00:25<00:00,  5.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.0450, Validation Accuracy: 56.08%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the Wav2Vec2 model and processor\n",
    "model = Wav2Vec2ForSequenceClassification.from_pretrained(\"facebook/wav2vec2-base\", num_labels=10)\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# Freeze parameters\n",
    "for param in model.wav2vec2.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Set the model to training mode\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer and loss function\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train and validate\n",
    "train_and_validate(model, train_loader, val_loader, optimizer, loss_fn, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6358bd9f-da5f-4491-beaf-de960d85da40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|███████████████████████████████████████████████████████████████████████████| 375/375 [00:56<00:00,  6.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 56.57%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf1dd351-5851-4616-bf58-57772a7b9280",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"seq-class-head.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d5c4ee-e98d-4fac-90a7-a5af29f36f2a",
   "metadata": {},
   "source": [
    "### Training fine-tuned model (all parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d13adf13-664e-4e4a-ad9d-4c006f3bdd1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Training Epoch 1: 100%|█████████████████████████████████████████████████████████████| 1350/1350 [04:36<00:00,  4.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5206, Train Accuracy: 93.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 1: 100%|█████████████████████████████████████████████████████████████| 150/150 [00:21<00:00,  6.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0689, Validation Accuracy: 99.46%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the Wav2Vec2 model \n",
    "model2 = Wav2Vec2ForSequenceClassification.from_pretrained(\"facebook/wav2vec2-base\", num_labels=10)  # Assuming 10 labels (digits 0-9)\n",
    "model2.gradient_checkpointing_enable()\n",
    "model2.train()\n",
    "\n",
    "# Optimizer and loss function\n",
    "optimizer = AdamW(model2.parameters(), lr=1e-5)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "model2.to(device)\n",
    "\n",
    "# Train and validate\n",
    "train_and_validate(model2, train_loader, val_loader, optimizer, loss_fn, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5bb9bf86-77b3-4eda-bfa4-a259b653590e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|███████████████████████████████████████████████████████████████████████████| 375/375 [00:54<00:00,  6.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 99.77%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_model(model2, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b109dccc-eafb-4edc-8eec-6731077d87d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model2.state_dict(), \"seq-class-fine.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
